# Hadoop-02: Cluster Setup

## 1. What Is a Hadoop Cluster?

A **Hadoop cluster** is a group of machines working together to run Hadoop services.

For learning, Hadoop supports **three deployment modes**:

| Mode                   | Description                 | Use Case               |
| ---------------------- | --------------------------- | ---------------------- |
| Local                  | Single JVM, no HDFS         | Testing only           |
| **Pseudo-Distributed** | Single machine, all daemons | **Best for beginners** |
| Fully Distributed      | Multiple machines           | Production             |

ðŸ‘‰ In this tutorial, we use **pseudo-distributed mode**.

### 1.1 Cluster Planning

A successful deployment requires a clear map of which services run on which hardware.

In this guide, we will build a 3-node cluster. We will use **hadoop01** as our master node, **hadoop02** as our Resource Manager, and **hadoop03** for backup services.

| **Node Name** | **IP Address** | **Services / Roles**                              |
| ------------- | -------------- | ------------------------------------------------- |
| **hadoop01**  | 192.168.184.31 | JDK 1.8, NameNode, DataNode, NodeManager          |
| **hadoop02**  | 192.168.184.32 | JDK 1.8, ResourceManager, DataNode, NodeManager   |
| **hadoop03**  | 192.168.184.33 | JDK 1.8, DataNode, NodeManager, SecondaryNameNode |

æ³¨ï¼šä¸Šè¡¨æ˜¯æ¯•ä¸šå®žä¹ çš„Hadoopé›†ç¾¤ã€‚

## 2. System Requirements

### Hardware

-   OS: Linux or macOS (Linux recommended)
-   RAM: â‰¥ 4 GB
-   Disk: â‰¥ 20 GB

### Software

-   Java 8 or 11
-   Hadoop 3.x
-   SSH (localhost access)

------

## 3. Step 1 â€“ Install Java

### Check Java

```bash
java -version
```

If not installed:

**Ubuntu / Debian**

```bash
sudo apt update
sudo apt install openjdk-11-jdk
```

**macOS (Homebrew)**

```bash
brew install openjdk@11
```

### Set JAVA_HOME

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

Verify:

```bash
echo $JAVA_HOME
```

------

## 4. Step 2 â€“ Install Hadoop

### Download Hadoop

```bash
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
```

### Extract

```bash
tar -xvzf hadoop-3.3.6.tar.gz
# Rename
mv hadoop-3.3.6 ~/hadoop
```

## 5. Step 3 â€“ Configure Environment Variables

Edit `~/.bashrc` or `~/.zshrc`:

```bash
export HADOOP_HOME=~/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Define root as the user for all Hadoop services
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_JOURNALNODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

Reload:

```bash
source ~/.bashrc
```

Test:

```bash
hadoop version
```

------

## 6. Step 4 â€“ Enable SSH (Important)

Hadoop requires **passwordless SSH**, even for localhost.

### Generate SSH Key

```bash
ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa
# Press Enter 3 times for default settings
```

-   `-t` - type

### Copy Key

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
```

or:

```
ssh-copy-id HOST
```

Test:

```bash
ssh localhost
```

------

## 7. Step 5 â€“ Configure Hadoop Files

Hadoop config files are in:

```bash
$HADOOP_HOME/etc/hadoop
```

------

### 7.1 core-site.xml (The Core)

This tells Hadoop where the **NameNode** is and where to store data.

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

example:

```xml
<configuration>
    <!-- æŒ‡å®š NameNode çš„åœ°å€ -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:8020</value>
    </property>

    <!-- æŒ‡å®š Hadoop æ•°æ®çš„å­˜å‚¨ç›®å½• -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/soft/hadoop/data</value>
    </property>

    <!-- é…ç½® HDFS Web ç™»å½•ä½¿ç”¨çš„é™æ€ç”¨æˆ·ä¸º root -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>

    <!-- é…ç½® root (superUser) å…è®¸é€šè¿‡ä»£ç†è®¿é—®çš„ä¸»æœºèŠ‚ç‚¹ -->
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>

    <!-- é…ç½® root (superUser) å…è®¸é€šè¿‡ä»£ç†ç”¨æˆ·æ‰€å±žç»„ -->
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>

    <!-- é…ç½® root (superUser) å…è®¸é€šè¿‡ä»£ç†çš„ç”¨æˆ· -->
    <property>
        <name>hadoop.proxyuser.root.users</name>
        <value>*</value>
    </property>
</configuration>
```

### 7.2 hdfs-site.xml (The Storage)

Defines web addresses and data redundancy.

```xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///home/youruser/hadoop_data/namenode</value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///home/youruser/hadoop_data/datanode</value>
  </property>
</configuration>
```

Create directories:

```bash
mkdir -p ~/hadoop_data/namenode
mkdir -p ~/hadoop_data/datanode
```

example:

```xml
<configuration>
    <!-- NN Web ç«¯è®¿é—®åœ°å€ -->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop01:9870</value>
    </property>

    <!-- 2NN Web ç«¯è®¿é—®åœ°å€ -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop03:9868</value>
    </property>

    <!-- æµ‹è¯•çŽ¯å¢ƒæŒ‡å®š HDFS å‰¯æœ¬æ•°é‡ä¸º 1 -->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

### 7.3 mapred-site.xml (The Engine)

Tells MapReduce to run on YARN.

```bash
cp mapred-site.xml.template mapred-site.xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

example:

```xml
<configuration>
    <!-- æŒ‡å®š MapReduce ç¨‹åºè¿è¡Œåœ¨ YARN ä¸Š -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <!-- åŽ†å²æœåŠ¡å™¨ç«¯åœ°å€ -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>hadoop01:10020</value>
    </property>

    <!-- åŽ†å²æœåŠ¡å™¨ Web ç«¯åœ°å€ -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>hadoop01:19888</value>
    </property>
</configuration>
```

### 7.4 yarn-site.xml (The Manager)

Designates the Resource Manager and sets memory limits.

```xml
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
```

example:

```xml
<configuration>
    <!-- æŒ‡å®š MR ä½¿ç”¨ shuffle æœåŠ¡ -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <!-- æŒ‡å®š ResourceManager çš„åœ°å€ -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>node1</value>
    </property>

    <!-- çŽ¯å¢ƒå˜é‡çš„ç»§æ‰¿ -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>
            JAVA_HOME,
            HADOOP_COMMON_HOME,
            HADOOP_HDFS_HOME,
            HADOOP_CONF_DIR,
            CLASSPATH_PREPEND_DISTCACHE,
            HADOOP_YARN_HOME,
            HADOOP_MAPRED_HOME
        </value>
    </property>

    <!-- Yarn å•ä¸ªå®¹å™¨å…è®¸åˆ†é…çš„æœ€å° / æœ€å¤§å†…å­˜ -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>

    <!-- Yarn å®¹å™¨å¯ç®¡ç†çš„ç‰©ç†å†…å­˜å¤§å° -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
    </property>

    <!-- å…³é—­ Yarn å¯¹ç‰©ç†å†…å­˜å’Œè™šæ‹Ÿå†…å­˜çš„é™åˆ¶æ£€æŸ¥ -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>

    <!-- å¼€å¯æ—¥å¿—èšé›†åŠŸèƒ½ -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>

    <!-- è®¾ç½®æ—¥å¿—èšé›†æœåŠ¡å™¨åœ°å€ -->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://hadoop01:19888/jobhistory/logs</value>
    </property>

    <!-- è®¾ç½®æ—¥å¿—ä¿ç•™æ—¶é—´ä¸º 7 å¤© -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>
```

### 7.5 workders

List your worker nodes: `vim workers`.

```
hadoop01
hadoop02
hadoop03
```

### 7.6 hadoop-env.sh

```bash
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
```

## 8. Step 6 â€“ Format NameNode (One Time Only)

```bash
# Run ONLY on hadoop01 the first time
hdfs namenode -format
```

âš ï¸ **Do this only once**.

## 9. Step 7 â€“ Start Hadoop Cluster

```bash
start-dfs.sh
# Run on hadoop02
start-yarn.sh
```

Verify:

```bash
jps
```

Expected output:

```
NameNode
DataNode
SecondaryNameNode
ResourceManager
NodeManager
```

## 10. Step 8 â€“ Web UIs

| Service         | URL                                             |
| --------------- | ----------------------------------------------- |
| NameNode        | [http://localhost:9870](http://localhost:9870/) |
| ResourceManager | [http://localhost:8088](http://localhost:8088/) |

------

## 11. Step 9 â€“ Test HDFS

### Create Directory

```bash
hdfs dfs -mkdir /input
```

### Upload File

```bash
hdfs dfs -put README.txt /input
```

### List Files

```bash
hdfs dfs -ls /input
```

------

## 12. Step 10 â€“ Run a Sample MapReduce Job

```bash
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output
```

View result:

```bash
hdfs dfs -cat /output/part-r-00000
```

------

## 13. Stop Hadoop

```bash
stop-yarn.sh
stop-dfs.sh
```

## 14. Automation Script (`hdp.sh`)

Create a script in `/root/bin/hdp.sh` to start/stop everything at once.

```Bash
#!/bin/bash

# Check if at least one argument is provided
if [ $# -lt 1 ]; then
    echo "No Args Input..."
    exit 1
fi

case $1 in
    "start")
        echo "=================== Starting Hadoop Cluster ==================="

        echo "--------------- Starting HDFS ---------------"
        ssh hadoop01 "start-dfs.sh"

        echo "--------------- Starting YARN ---------------"
        ssh hadoop02 "start-yarn.sh"

        echo "--------------- Starting HistoryServer ---------------"
        ssh hadoop01 "mapred --daemon start historyserver"
        ;;

    "stop")
        echo "=================== Stopping Hadoop Cluster ==================="

        echo "--------------- Stopping HistoryServer ---------------"
        ssh hadoop01 "mapred --daemon stop historyserver"

        echo "--------------- Stopping YARN ---------------"
        ssh hadoop02 "stop-yarn.sh"

        echo "--------------- Stopping HDFS ---------------"
        ssh hadoop01 "stop-dfs.sh"
        ;;

    *)
        echo "Input Args Error..."
        ;;
esac
```

**Don't forget to give it permission**: `chmod +x hdp.sh`. You can now verify everything is running by typing `jps` on each node.

```sh
 hdp.sh stop
 hdp.sh start
```

## 15. Common Beginner Errors

### âŒ SSH asks for password

âœ” Passwordless SSH not configured correctly

### âŒ JAVA_HOME error

âœ” Java path incorrect in `hadoop-env.sh`

### âŒ Permission denied

âœ” HDFS directories owned by wrong user

## 16. From Here to Real Clusters

Once comfortable, next steps are:

1.  Multi-node Hadoop cluster
2.  Hive installation
3.  Spark on YARN
4.  HDFS performance tuning

## 17. Mental Model

>   One machine pretending to be a cluster
>   All Hadoop daemons talking via SSH
>   Same concepts as production