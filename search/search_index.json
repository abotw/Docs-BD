{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Big Data Docs","text":"<ul> <li>\u53a6\u95e8\u5927\u5b66\u6570\u636e\u5e93\u5b9e\u9a8c\u5ba4</li> </ul>"},{"location":"datax/","title":"DataX","text":""},{"location":"datax/#1-what-is-datax","title":"1. What Is DataX?","text":"<p>DataX is an open-source data synchronization tool developed by Alibaba. It is mainly used to move data between different data sources efficiently and reliably.</p> <p>Typical use cases include:</p> <ul> <li>MySQL \u2192 HDFS</li> <li>MySQL \u2192 Hive</li> <li>Oracle \u2192 MySQL</li> <li>PostgreSQL \u2192 HBase</li> <li>CSV / TXT \u2192 Database</li> <li>Database \u2192 Database</li> </ul> <p>\ud83d\udc49 In short: DataX = batch data migration / synchronization tool</p>"},{"location":"datax/#2-core-features","title":"2. Core Features","text":"<ul> <li>Plugin-based architecture (easy to extend)</li> <li>Supports many data sources</li> <li>High performance (parallel execution)</li> <li>Simple JSON configuration</li> <li>Suitable for offline / batch data sync</li> </ul> <p>\u26a0\ufe0f Note: DataX is not designed for real-time streaming (use Kafka / Flink / Debezium for that).</p>"},{"location":"datax/#3-how-datax-works-high-level","title":"3. How DataX Works (High-Level)","text":"<p>DataX uses a Reader \u2192 Channel \u2192 Writer model.</p> <pre><code>[ Source ] --&gt; Reader --&gt; Channel --&gt; Writer --&gt; [ Target ]\n</code></pre>"},{"location":"datax/#components","title":"Components","text":"<ol> <li>Reader<ul> <li>Reads data from the source</li> <li>Example: MySQLReader, HDFSReader</li> </ul> </li> <li>Channel<ul> <li>Data transmission pipeline</li> <li>Controls concurrency and speed</li> </ul> </li> <li>Writer<ul> <li>Writes data to the target</li> <li>Example: MySQLWriter, HiveWriter</li> </ul> </li> </ol>"},{"location":"datax/#4-supported-data-sources-common-ones","title":"4. Supported Data Sources (Common Ones)","text":"Type Examples Relational DB MySQL, Oracle, PostgreSQL, SQL Server Big Data HDFS, Hive, HBase NoSQL MongoDB File TXT, CSV Others Elasticsearch <p>Each data source is implemented as a Reader or Writer plugin.</p>"},{"location":"datax/#5-installation","title":"5. Installation","text":""},{"location":"datax/#51-environment-requirements","title":"5.1 Environment Requirements","text":"<ul> <li>Linux or macOS</li> <li>Python 2.7 or Python 3.x</li> <li>Java JDK 8+</li> </ul> <p>Check Java:</p> <pre><code>java -version\n</code></pre>"},{"location":"datax/#52-download-datax","title":"5.2 Download DataX","text":"<pre><code>wget https://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz\ntar -zxvf datax.tar.gz\ncd datax\n</code></pre> <p>Directory structure:</p> <pre><code>datax/\n\u251c\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 datax.py\n\u251c\u2500\u2500 conf/\n\u251c\u2500\u2500 job/\n\u251c\u2500\u2500 plugin/\n\u2514\u2500\u2500 lib/\n</code></pre>"},{"location":"datax/#6-basic-usage","title":"6. Basic Usage","text":""},{"location":"datax/#command-format","title":"Command Format","text":"<pre><code>python bin/datax.py job/job.json\n</code></pre> <p>Where <code>job.json</code> is the job configuration file.</p>"},{"location":"datax/#7-datax-job-configuration-explained","title":"7. DataX Job Configuration Explained","text":"<p>A DataX job is defined using JSON.</p>"},{"location":"datax/#basic-structure","title":"Basic Structure","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {},\n    \"content\": [\n      {\n        \"reader\": {},\n        \"writer\": {}\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"datax/#8-example-mysql-mysql","title":"8. Example: MySQL \u2192 MySQL","text":""},{"location":"datax/#81-job-file-mysql_to_mysqljson","title":"8.1 Job File: <code>mysql_to_mysql.json</code>","text":"<pre><code>{\n  \"job\": {\n    \"setting\": {\n      \"speed\": {\n        \"channel\": 3\n      }\n    },\n    \"content\": [\n      {\n        \"reader\": {\n          \"name\": \"mysqlreader\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"123456\",\n            \"column\": [\"id\", \"name\", \"age\"],\n            \"connection\": [\n              {\n                \"table\": [\"user\"],\n                \"jdbcUrl\": [\n                  \"jdbc:mysql://localhost:3306/source_db\"\n                ]\n              }\n            ]\n          }\n        },\n        \"writer\": {\n          \"name\": \"mysqlwriter\",\n          \"parameter\": {\n            \"username\": \"root\",\n            \"password\": \"123456\",\n            \"column\": [\"id\", \"name\", \"age\"],\n            \"connection\": [\n              {\n                \"table\": [\"user_copy\"],\n                \"jdbcUrl\": \"jdbc:mysql://localhost:3306/target_db\"\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"datax/#82-run-the-job","title":"8.2 Run the Job","text":"<pre><code>python bin/datax.py job/mysql_to_mysql.json\n</code></pre> <p>If successful, you will see:</p> <pre><code>Task start time...\nTask end time...\nTotal records: xxxx\n</code></pre>"},{"location":"datax/#9-important-configuration-options","title":"9. Important Configuration Options","text":""},{"location":"datax/#91-speed-control","title":"9.1 Speed Control","text":"<p>Limit parallelism:</p> <pre><code>\"setting\": {\n  \"speed\": {\n    \"channel\": 2\n  }\n}\n</code></pre> <p>Limit bytes per second:</p> <pre><code>\"speed\": {\n  \"byte\": 1048576\n}\n</code></pre>"},{"location":"datax/#92-column-selection","title":"9.2 Column Selection","text":"<ul> <li>All columns:</li> </ul> <pre><code>\"column\": [\"*\"]\n</code></pre> <ul> <li>Specific columns:</li> </ul> <pre><code>\"column\": [\"id\", \"name\"]\n</code></pre>"},{"location":"datax/#93-where-condition-mysql-reader","title":"9.3 Where Condition (MySQL Reader)","text":"<pre><code>\"where\": \"id &gt; 100\"\n</code></pre>"},{"location":"datax/#10-common-plugins","title":"10. Common Plugins","text":"Plugin Description mysqlreader Read data from MySQL mysqlwriter Write data to MySQL hdfsreader Read from HDFS hdfswriter Write to HDFS hivereader Read from Hive hivewriter Write to Hive <p>Plugins are located in:</p> <pre><code>plugin/reader/\nplugin/writer/\n</code></pre>"},{"location":"datax/#11-error-handling-logs","title":"11. Error Handling &amp; Logs","text":""},{"location":"datax/#log-location","title":"Log Location","text":"<pre><code>log/\n</code></pre> <p>Common errors:</p> <ul> <li>Database connection failure</li> <li>Column mismatch</li> <li>Permission issues</li> <li>Character encoding problems</li> </ul>"},{"location":"datax/#12-performance-tuning-tips","title":"12. Performance Tuning Tips","text":"<ul> <li>Increase <code>channel</code> carefully</li> <li>Avoid <code>select *</code> on large tables</li> <li>Use <code>where</code> for incremental sync</li> <li>Ensure indexes exist on source tables</li> <li>Avoid running too many jobs at the same time</li> </ul>"},{"location":"datax/#13-when-should-you-use-datax","title":"13. When Should You Use DataX?","text":"<p>\u2705 Good for:</p> <ul> <li>Offline batch migration</li> <li>Periodic data synchronization</li> <li>One-time data transfer</li> </ul> <p>\u274c Not suitable for:</p> <ul> <li>Real-time streaming</li> <li>Low-latency CDC</li> </ul>"},{"location":"datax/#14-learning-path-recommendation","title":"14. Learning Path Recommendation","text":"<ol> <li>Understand Reader / Writer model</li> <li>Practice MySQL \u2192 MySQL</li> <li>Try MySQL \u2192 HDFS / Hive</li> <li>Learn performance tuning</li> <li>Read plugin source code (advanced)</li> </ol>"},{"location":"datax/#15-summary","title":"15. Summary","text":"<ul> <li>DataX is a powerful batch data sync tool</li> <li>Uses JSON-based configuration</li> <li>Reader + Channel + Writer architecture</li> <li>Best for offline data integration</li> </ul>"},{"location":"env-mysql/","title":"ENV: MySQL","text":"<p>This document provides a step-by-step procedure for installing MySQL 8.0.31 on a Linux system (e.g., CentOS 7) using RPM packages and an automated configuration script.</p>"},{"location":"env-mysql/#1-preparation-package-upload","title":"1. Preparation &amp; Package Upload","text":"<p>Before starting the installation, you must prepare the directory structure and upload the necessary files.</p> <p>Create the target directory:</p> <pre><code>mkdir -p /usr/local/soft/mysql\ncd /usr/local/soft/mysql/\n</code></pre> <p>Upload the following files to the directory:</p> <ul> <li>Script: <code>install_mysql.sh</code></li> <li>RPM Packages:<ul> <li><code>mysql-community-common-8.0.31-1.el7.x86_64.rpm</code></li> <li><code>mysql-community-client-plugins-8.0.31-1.el7.x86_64.rpm</code></li> <li><code>mysql-community-libs-8.0.31-1.el7.x86_64.rpm</code></li> <li><code>mysql-community-libs-compat-8.0.31-1.el7.x86_64.rpm</code></li> <li><code>mysql-community-icu-data-files-8.0.31-1.el7.x86_64.rpm</code></li> <li><code>mysql-community-client-8.0.31-1.el7.x86_64.rpm</code></li> <li><code>mysql-community-server-8.0.31-1.el7.x86_64.rpm</code></li> </ul> </li> <li>Java Connector: <code>mysql-connector-j-8.0.31.jar</code></li> </ul>"},{"location":"env-mysql/#2-pre-installation-cleanup","title":"2. Pre-installation Cleanup","text":"<p>To prevent conflicts, you must remove existing MySQL/MariaDB libraries and install necessary dependencies.</p> <ol> <li> <p>Remove conflicting libraries:</p> <pre><code>yum remove mysql-libs\n</code></pre> </li> <li> <p>Install dependencies:</p> <pre><code>yum -y install libaio autoconf\n</code></pre> </li> </ol>"},{"location":"env-mysql/#3-automated-installation-script","title":"3. Automated Installation Script","text":"<p>The <code>install_mysql.sh</code> script automates the environment cleanup, package installation, and initial security configuration.</p>"},{"location":"env-mysql/#script-content-overview","title":"Script Content Overview","text":"<p>The script performs the following critical tasks:</p> <ul> <li>Validation: Verifies root privileges and the presence of all 7 RPM packages.</li> <li>Force Cleanup: Stops existing services and wipes old data/logs (<code>/var/lib/mysql</code>, <code>/etc/my.cnf</code>).</li> <li>Installation: Installs the RPMs and starts the <code>mysqld</code> service.</li> <li>Password Policy Adjustment: Modifies <code>/etc/my.cnf</code> to allow simpler passwords (Length: 4, Policy: Low).</li> <li>Root Initialization:<ul> <li>Extracts the temporary password from logs.</li> <li>Resets the root password to <code>000000</code>.</li> <li>Enables remote root login (<code>host='%'</code>).</li> <li>Sets authentication to <code>mysql_native_password</code>.</li> </ul> </li> </ul>"},{"location":"env-mysql/#executing-the-installation","title":"Executing the Installation","text":"<p>Run the script from within the <code>/usr/local/soft/mysql</code> directory:</p> <pre><code>sh install_mysql.sh\n</code></pre>"},{"location":"env-mysql/#4-post-installation-summary","title":"4. Post-Installation Summary","text":"Setting Value Default Root Password <code>000000</code> Remote Access Enabled (<code>%</code>) Auth Plugin <code>mysql_native_password</code> Password Policy Length 4 / Low Policy"},{"location":"logs/","title":"Logs","text":"<ul> <li>2025-12-15 ~ 2026-01-09: \u6bd5\u4e1a\u5b9e\u4e60</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/","title":"Hadoop-01: Intro","text":"<p>A Gentle Introduction to Apache Hadoop</p>"},{"location":"Hadoop/hadoop-01-intro/#1-what-is-hadoop","title":"1. What Is Hadoop?","text":"<p>Apache Hadoop is an open-source framework designed to store and process very large datasets across many computers (clusters) in a reliable and scalable way.</p> <p>In simple terms:</p> <p>Hadoop lets you use many cheap machines to work together as one powerful system for big data.</p>"},{"location":"Hadoop/hadoop-01-intro/#2-why-do-we-need-hadoop","title":"2. Why Do We Need Hadoop?","text":"<p>Traditional systems struggle when data becomes:</p> <ul> <li>Too large (TBs or PBs)</li> <li>Too fast (logs, streams, events)</li> <li>Too complex (semi-structured, unstructured)</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#problems-with-traditional-databases","title":"Problems with Traditional Databases","text":"<ul> <li>Scale up (bigger server) instead of out</li> <li>Expensive hardware</li> <li>Limited fault tolerance</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#hadoops-solution","title":"Hadoop\u2019s Solution","text":"<ul> <li>Scale out using many machines</li> <li>Built-in fault tolerance</li> <li>Works well with massive data</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#3-hadoop-core-ideas-very-important","title":"3. Hadoop Core Ideas (Very Important)","text":"<p>Hadoop is built on three key ideas:</p>"},{"location":"Hadoop/hadoop-01-intro/#1-distributed-storage","title":"1\ufe0f\u20e3 Distributed Storage","text":"<p>Data is split and stored across many machines.</p>"},{"location":"Hadoop/hadoop-01-intro/#2-distributed-computing","title":"2\ufe0f\u20e3 Distributed Computing","text":"<p>Computation is sent to where the data lives, not the other way around.</p>"},{"location":"Hadoop/hadoop-01-intro/#3-fault-tolerance","title":"3\ufe0f\u20e3 Fault Tolerance","text":"<p>If a machine fails, Hadoop automatically recovers.</p>"},{"location":"Hadoop/hadoop-01-intro/#4-hadoop-ecosystem-overview","title":"4. Hadoop Ecosystem Overview","text":"<p>Hadoop is not a single tool. It is an ecosystem.</p> <pre><code>Hadoop Ecosystem\n\u2502\n\u251c\u2500\u2500 HDFS       (Storage)\n\u251c\u2500\u2500 YARN       (Resource Management)\n\u251c\u2500\u2500 MapReduce  (Computation)\n\u251c\u2500\u2500 Hive       (SQL on Hadoop)\n\u251c\u2500\u2500 HBase      (NoSQL Database)\n\u251c\u2500\u2500 Spark      (Fast Processing Engine)\n\u2514\u2500\u2500 Others...\n</code></pre> <p>For beginners, focus on three core components first.</p>"},{"location":"Hadoop/hadoop-01-intro/#5-hdfs-hadoop-distributed-file-system","title":"5. HDFS \u2013 Hadoop Distributed File System","text":""},{"location":"Hadoop/hadoop-01-intro/#what-is-hdfs","title":"What Is HDFS?","text":"<p>HDFS is Hadoop\u2019s distributed file system.</p> <p>It stores files by:</p> <ul> <li>Splitting them into blocks</li> <li>Distributing blocks across multiple machines</li> <li>Replicating blocks for safety</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#example","title":"Example","text":"<p>A 1GB file might be:</p> <ul> <li>Split into 8 blocks</li> <li>Stored on 8 different machines</li> <li>Each block copied 3 times</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#hdfs-architecture","title":"HDFS Architecture","text":""},{"location":"Hadoop/hadoop-01-intro/#key-roles","title":"Key Roles","text":"Role Description NameNode Metadata manager (file names, locations) DataNode Stores actual data blocks Secondary NameNode Assists with metadata checkpoints <p>\ud83d\udca1 Data flows through DataNodes, not the NameNode</p>"},{"location":"Hadoop/hadoop-01-intro/#why-hdfs-is-special","title":"Why HDFS Is Special","text":"<ul> <li>Optimized for large files</li> <li>High throughput, not low latency</li> <li>Write once, read many times</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#6-yarn-resource-management","title":"6. YARN \u2013 Resource Management","text":""},{"location":"Hadoop/hadoop-01-intro/#what-is-yarn","title":"What Is YARN?","text":"<p>YARN (Yet Another Resource Negotiator) manages:</p> <ul> <li>CPU</li> <li>Memory</li> <li>Task scheduling</li> </ul> <p>Think of YARN as the operating system of a Hadoop cluster.</p>"},{"location":"Hadoop/hadoop-01-intro/#yarn-components","title":"YARN Components","text":"Component Role ResourceManager Global resource controller NodeManager Manages resources on each node ApplicationMaster Manages a single application"},{"location":"Hadoop/hadoop-01-intro/#7-mapreduce-distributed-computing-model","title":"7. MapReduce \u2013 Distributed Computing Model","text":""},{"location":"Hadoop/hadoop-01-intro/#what-is-mapreduce","title":"What Is MapReduce?","text":"<p>MapReduce is a programming model for processing large datasets in parallel.</p> <p>It has two phases:</p> <pre><code>Input \u2192 Map \u2192 Shuffle \u2192 Reduce \u2192 Output\n</code></pre>"},{"location":"Hadoop/hadoop-01-intro/#map-phase","title":"Map Phase","text":"<ul> <li>Processes input data</li> <li>Produces key-value pairs</li> </ul> <p>Example:</p> <pre><code>Input: \"hello hadoop hello\"\nOutput:\nhello \u2192 1\nhadoop \u2192 1\nhello \u2192 1\n</code></pre>"},{"location":"Hadoop/hadoop-01-intro/#reduce-phase","title":"Reduce Phase","text":"<ul> <li>Aggregates values with the same key</li> </ul> <pre><code>hello \u2192 2\nhadoop \u2192 1\n</code></pre>"},{"location":"Hadoop/hadoop-01-intro/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Highly scalable</li> <li>Disk-based (slower than Spark)</li> <li>Very stable and reliable</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#8-hadoop-cluster-modes","title":"8. Hadoop Cluster Modes","text":""},{"location":"Hadoop/hadoop-01-intro/#1-local-mode","title":"1\ufe0f\u20e3 Local Mode","text":"<ul> <li>Single machine</li> <li>Used for learning and testing</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#2-pseudo-distributed-mode","title":"2\ufe0f\u20e3 Pseudo-Distributed Mode","text":"<ul> <li>One machine acts like a cluster</li> <li>Common for beginners</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#3-fully-distributed-mode","title":"3\ufe0f\u20e3 Fully Distributed Mode","text":"<ul> <li>Multiple machines</li> <li>Production environment</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#9-hadoop-vs-traditional-systems","title":"9. Hadoop vs Traditional Systems","text":"Feature Traditional DB Hadoop Data Size GBs TBs\u2013PBs Schema Fixed Schema-on-read Fault Tolerance Limited Built-in Cost High Lower Scalability Vertical Horizontal"},{"location":"Hadoop/hadoop-01-intro/#10-hadoop-vs-spark-quick-note","title":"10. Hadoop vs Spark (Quick Note)","text":"Hadoop MapReduce Spark Disk-based Memory-based Slower Faster Simple model Rich APIs Very stable Widely used today <p>\ud83d\udca1 In modern systems, Hadoop + Spark + Hive is very common.</p>"},{"location":"Hadoop/hadoop-01-intro/#11-typical-hadoop-use-cases","title":"11. Typical Hadoop Use Cases","text":"<ul> <li>Log analysis</li> <li>Data warehousing</li> <li>ETL pipelines</li> <li>Clickstream analysis</li> <li>Data lakes</li> </ul>"},{"location":"Hadoop/hadoop-01-intro/#12-learning-path-for-beginners","title":"12. Learning Path for Beginners","text":"<p>Recommended order:</p> <ol> <li>Linux basics</li> <li>HDFS commands</li> <li>Hadoop architecture</li> <li>YARN concepts</li> <li>MapReduce basics</li> <li>Hive (SQL on Hadoop)</li> <li>Spark (next step)</li> </ol>"},{"location":"Hadoop/hadoop-01-intro/#13-common-beginner-misconceptions","title":"13. Common Beginner Misconceptions","text":"<p>\u274c Hadoop is a database \u2714 Hadoop is a framework</p> <p>\u274c Hadoop is outdated \u2714 Hadoop is the foundation of modern big data</p> <p>\u274c Hadoop is only MapReduce \u2714 Hadoop includes HDFS + YARN + more</p>"},{"location":"Hadoop/hadoop-01-intro/#14-simple-mental-model","title":"14. Simple Mental Model","text":"<p>Hadoop =   HDFS (storage) + YARN (resource manager) + Compute engine</p>"},{"location":"Hadoop/hadoop-01-intro/#15-what-should-you-learn-next","title":"15. What Should You Learn Next?","text":"<p>If you want to continue, good next topics are:</p> <ul> <li>HDFS command line usage</li> <li>Installing Hadoop in pseudo-distributed mode</li> <li>Hive SQL basics</li> <li>Spark vs MapReduce</li> </ul>"},{"location":"Hadoop/hadoop-02-cluster/","title":"Hadoop-02: Cluster Setup","text":""},{"location":"Hadoop/hadoop-02-cluster/#1-what-is-a-hadoop-cluster","title":"1. What Is a Hadoop Cluster?","text":"<p>A Hadoop cluster is a group of machines working together to run Hadoop services.</p> <p>For learning, Hadoop supports three deployment modes:</p> Mode Description Use Case Local Single JVM, no HDFS Testing only Pseudo-Distributed Single machine, all daemons Best for beginners Fully Distributed Multiple machines Production <p>\ud83d\udc49 In this tutorial, we use pseudo-distributed mode.</p>"},{"location":"Hadoop/hadoop-02-cluster/#11-cluster-planning","title":"1.1 Cluster Planning","text":"<p>A successful deployment requires a clear map of which services run on which hardware.</p> <p>In this guide, we will build a 3-node cluster. We will use hadoop01 as our master node, hadoop02 as our Resource Manager, and hadoop03 for backup services.</p> Node Name IP Address Services / Roles hadoop01 192.168.184.31 JDK 1.8, NameNode, DataNode, NodeManager hadoop02 192.168.184.32 JDK 1.8, ResourceManager, DataNode, NodeManager hadoop03 192.168.184.33 JDK 1.8, DataNode, NodeManager, SecondaryNameNode <p>\u6ce8\uff1a\u4e0a\u8868\u662f\u6bd5\u4e1a\u5b9e\u4e60\u7684Hadoop\u96c6\u7fa4\u3002</p>"},{"location":"Hadoop/hadoop-02-cluster/#2-system-requirements","title":"2. System Requirements","text":""},{"location":"Hadoop/hadoop-02-cluster/#hardware","title":"Hardware","text":"<ul> <li>OS: Linux or macOS (Linux recommended)</li> <li>RAM: \u2265 4 GB</li> <li>Disk: \u2265 20 GB</li> </ul>"},{"location":"Hadoop/hadoop-02-cluster/#software","title":"Software","text":"<ul> <li>Java 8 or 11</li> <li>Hadoop 3.x</li> <li>SSH (localhost access)</li> </ul>"},{"location":"Hadoop/hadoop-02-cluster/#3-step-1-install-java","title":"3. Step 1 \u2013 Install Java","text":""},{"location":"Hadoop/hadoop-02-cluster/#check-java","title":"Check Java","text":"<pre><code>java -version\n</code></pre> <p>If not installed:</p> <p>Ubuntu / Debian</p> <pre><code>sudo apt update\nsudo apt install openjdk-11-jdk\n</code></pre> <p>macOS (Homebrew)</p> <pre><code>brew install openjdk@11\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#set-java_home","title":"Set JAVA_HOME","text":"<pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n</code></pre> <p>Verify:</p> <pre><code>echo $JAVA_HOME\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#4-step-2-install-hadoop","title":"4. Step 2 \u2013 Install Hadoop","text":""},{"location":"Hadoop/hadoop-02-cluster/#download-hadoop","title":"Download Hadoop","text":"<pre><code>wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#extract","title":"Extract","text":"<pre><code>tar -xvzf hadoop-3.3.6.tar.gz\n# Rename\nmv hadoop-3.3.6 ~/hadoop\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#5-step-3-configure-environment-variables","title":"5. Step 3 \u2013 Configure Environment Variables","text":"<p>Edit <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code>export HADOOP_HOME=~/hadoop\nexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n\n# Define root as the user for all Hadoop services\nexport HDFS_NAMENODE_USER=root\nexport HDFS_DATANODE_USER=root\nexport HDFS_JOURNALNODE_USER=root\nexport HDFS_SECONDARYNAMENODE_USER=root\nexport YARN_RESOURCEMANAGER_USER=root\nexport YARN_NODEMANAGER_USER=root\n</code></pre> <p>Reload:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Test:</p> <pre><code>hadoop version\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#6-step-4-enable-ssh-important","title":"6. Step 4 \u2013 Enable SSH (Important)","text":"<p>Hadoop requires passwordless SSH, even for localhost.</p>"},{"location":"Hadoop/hadoop-02-cluster/#generate-ssh-key","title":"Generate SSH Key","text":"<pre><code>ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n# Press Enter 3 times for default settings\n</code></pre> <ul> <li><code>-t</code> - type</li> </ul>"},{"location":"Hadoop/hadoop-02-cluster/#copy-key","title":"Copy Key","text":"<pre><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\n</code></pre> <p>or:</p> <pre><code>ssh-copy-id HOST\n</code></pre> <p>Test:</p> <pre><code>ssh localhost\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#7-step-5-configure-hadoop-files","title":"7. Step 5 \u2013 Configure Hadoop Files","text":"<p>Hadoop config files are in:</p> <pre><code>$HADOOP_HOME/etc/hadoop\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#71-core-sitexml-the-core","title":"7.1 core-site.xml (The Core)","text":"<p>This tells Hadoop where the NameNode is and where to store data.</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;fs.defaultFS&lt;/name&gt;\n    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>example:</p> <pre><code>&lt;configuration&gt;\n    &lt;!-- \u6307\u5b9a NameNode \u7684\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://hadoop01:8020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u6307\u5b9a Hadoop \u6570\u636e\u7684\u5b58\u50a8\u76ee\u5f55 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;/usr/local/soft/hadoop/data&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u914d\u7f6e HDFS Web \u767b\u5f55\u4f7f\u7528\u7684\u9759\u6001\u7528\u6237\u4e3a root --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;\n        &lt;value&gt;root&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u914d\u7f6e root (superUser) \u5141\u8bb8\u901a\u8fc7\u4ee3\u7406\u8bbf\u95ee\u7684\u4e3b\u673a\u8282\u70b9 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;\n        &lt;value&gt;*&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u914d\u7f6e root (superUser) \u5141\u8bb8\u901a\u8fc7\u4ee3\u7406\u7528\u6237\u6240\u5c5e\u7ec4 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;\n        &lt;value&gt;*&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u914d\u7f6e root (superUser) \u5141\u8bb8\u901a\u8fc7\u4ee3\u7406\u7684\u7528\u6237 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.proxyuser.root.users&lt;/name&gt;\n        &lt;value&gt;*&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#72-hdfs-sitexml-the-storage","title":"7.2 hdfs-site.xml (The Storage)","text":"<p>Defines web addresses and data redundancy.</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;dfs.replication&lt;/name&gt;\n    &lt;value&gt;1&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n    &lt;value&gt;file:///home/youruser/hadoop_data/namenode&lt;/value&gt;\n  &lt;/property&gt;\n\n  &lt;property&gt;\n    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n    &lt;value&gt;file:///home/youruser/hadoop_data/datanode&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>Create directories:</p> <pre><code>mkdir -p ~/hadoop_data/namenode\nmkdir -p ~/hadoop_data/datanode\n</code></pre> <p>example:</p> <pre><code>&lt;configuration&gt;\n    &lt;!-- NN Web \u7aef\u8bbf\u95ee\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;\n        &lt;value&gt;hadoop01:9870&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- 2NN Web \u7aef\u8bbf\u95ee\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n        &lt;value&gt;hadoop03:9868&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u6d4b\u8bd5\u73af\u5883\u6307\u5b9a HDFS \u526f\u672c\u6570\u91cf\u4e3a 1 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#73-mapred-sitexml-the-engine","title":"7.3 mapred-site.xml (The Engine)","text":"<p>Tells MapReduce to run on YARN.</p> <pre><code>cp mapred-site.xml.template mapred-site.xml\n&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n    &lt;value&gt;yarn&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>example:</p> <pre><code>&lt;configuration&gt;\n    &lt;!-- \u6307\u5b9a MapReduce \u7a0b\u5e8f\u8fd0\u884c\u5728 YARN \u4e0a --&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u5386\u53f2\u670d\u52a1\u5668\u7aef\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;\n        &lt;value&gt;hadoop01:10020&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u5386\u53f2\u670d\u52a1\u5668 Web \u7aef\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;\n        &lt;value&gt;hadoop01:19888&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#74-yarn-sitexml-the-manager","title":"7.4 yarn-site.xml (The Manager)","text":"<p>Designates the Resource Manager and sets memory limits.</p> <pre><code>&lt;configuration&gt;\n  &lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n  &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre> <p>example:</p> <pre><code>&lt;configuration&gt;\n    &lt;!-- \u6307\u5b9a MR \u4f7f\u7528 shuffle \u670d\u52a1 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u6307\u5b9a ResourceManager \u7684\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;node1&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u73af\u5883\u53d8\u91cf\u7684\u7ee7\u627f --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;\n        &lt;value&gt;\n            JAVA_HOME,\n            HADOOP_COMMON_HOME,\n            HADOOP_HDFS_HOME,\n            HADOOP_CONF_DIR,\n            CLASSPATH_PREPEND_DISTCACHE,\n            HADOOP_YARN_HOME,\n            HADOOP_MAPRED_HOME\n        &lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Yarn \u5355\u4e2a\u5bb9\u5668\u5141\u8bb8\u5206\u914d\u7684\u6700\u5c0f / \u6700\u5927\u5185\u5b58 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;\n        &lt;value&gt;512&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;\n        &lt;value&gt;4096&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- Yarn \u5bb9\u5668\u53ef\u7ba1\u7406\u7684\u7269\u7406\u5185\u5b58\u5927\u5c0f --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;\n        &lt;value&gt;4096&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u5173\u95ed Yarn \u5bf9\u7269\u7406\u5185\u5b58\u548c\u865a\u62df\u5185\u5b58\u7684\u9650\u5236\u68c0\u67e5 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n        &lt;value&gt;false&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u5f00\u542f\u65e5\u5fd7\u805a\u96c6\u529f\u80fd --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u8bbe\u7f6e\u65e5\u5fd7\u805a\u96c6\u670d\u52a1\u5668\u5730\u5740 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.log.server.url&lt;/name&gt;\n        &lt;value&gt;http://hadoop01:19888/jobhistory/logs&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;!-- \u8bbe\u7f6e\u65e5\u5fd7\u4fdd\u7559\u65f6\u95f4\u4e3a 7 \u5929 --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;\n        &lt;value&gt;604800&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#75-workders","title":"7.5 workders","text":"<p>List your worker nodes: <code>vim workers</code>.</p> <pre><code>hadoop01\nhadoop02\nhadoop03\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#76-hadoop-envsh","title":"7.6 hadoop-env.sh","text":"<pre><code>export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#8-step-6-format-namenode-one-time-only","title":"8. Step 6 \u2013 Format NameNode (One Time Only)","text":"<pre><code># Run ONLY on hadoop01 the first time\nhdfs namenode -format\n</code></pre> <p>\u26a0\ufe0f Do this only once.</p>"},{"location":"Hadoop/hadoop-02-cluster/#9-step-7-start-hadoop-cluster","title":"9. Step 7 \u2013 Start Hadoop Cluster","text":"<pre><code>start-dfs.sh\n# Run on hadoop02\nstart-yarn.sh\n</code></pre> <p>Verify:</p> <pre><code>jps\n</code></pre> <p>Expected output:</p> <pre><code>NameNode\nDataNode\nSecondaryNameNode\nResourceManager\nNodeManager\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#10-step-8-web-uis","title":"10. Step 8 \u2013 Web UIs","text":"Service URL NameNode http://localhost:9870 ResourceManager http://localhost:8088"},{"location":"Hadoop/hadoop-02-cluster/#11-step-9-test-hdfs","title":"11. Step 9 \u2013 Test HDFS","text":""},{"location":"Hadoop/hadoop-02-cluster/#create-directory","title":"Create Directory","text":"<pre><code>hdfs dfs -mkdir /input\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#upload-file","title":"Upload File","text":"<pre><code>hdfs dfs -put README.txt /input\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#list-files","title":"List Files","text":"<pre><code>hdfs dfs -ls /input\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#12-step-10-run-a-sample-mapreduce-job","title":"12. Step 10 \u2013 Run a Sample MapReduce Job","text":"<pre><code>hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar wordcount /input /output\n</code></pre> <p>View result:</p> <pre><code>hdfs dfs -cat /output/part-r-00000\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#13-stop-hadoop","title":"13. Stop Hadoop","text":"<pre><code>stop-yarn.sh\nstop-dfs.sh\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#14-automation-script-hdpsh","title":"14. Automation Script (<code>hdp.sh</code>)","text":"<p>Create a script in <code>/root/bin/hdp.sh</code> to start/stop everything at once.</p> <pre><code>#!/bin/bash\n\n# Check if at least one argument is provided\nif [ $# -lt 1 ]; then\n    echo \"No Args Input...\"\n    exit 1\nfi\n\ncase $1 in\n    \"start\")\n        echo \"=================== Starting Hadoop Cluster ===================\"\n\n        echo \"--------------- Starting HDFS ---------------\"\n        ssh hadoop01 \"start-dfs.sh\"\n\n        echo \"--------------- Starting YARN ---------------\"\n        ssh hadoop02 \"start-yarn.sh\"\n\n        echo \"--------------- Starting HistoryServer ---------------\"\n        ssh hadoop01 \"mapred --daemon start historyserver\"\n        ;;\n\n    \"stop\")\n        echo \"=================== Stopping Hadoop Cluster ===================\"\n\n        echo \"--------------- Stopping HistoryServer ---------------\"\n        ssh hadoop01 \"mapred --daemon stop historyserver\"\n\n        echo \"--------------- Stopping YARN ---------------\"\n        ssh hadoop02 \"stop-yarn.sh\"\n\n        echo \"--------------- Stopping HDFS ---------------\"\n        ssh hadoop01 \"stop-dfs.sh\"\n        ;;\n\n    *)\n        echo \"Input Args Error...\"\n        ;;\nesac\n</code></pre> <p>Don't forget to give it permission: <code>chmod +x hdp.sh</code>. You can now verify everything is running by typing <code>jps</code> on each node.</p> <pre><code> hdp.sh stop\n hdp.sh start\n</code></pre>"},{"location":"Hadoop/hadoop-02-cluster/#15-common-beginner-errors","title":"15. Common Beginner Errors","text":""},{"location":"Hadoop/hadoop-02-cluster/#ssh-asks-for-password","title":"\u274c SSH asks for password","text":"<p>\u2714 Passwordless SSH not configured correctly</p>"},{"location":"Hadoop/hadoop-02-cluster/#java_home-error","title":"\u274c JAVA_HOME error","text":"<p>\u2714 Java path incorrect in <code>hadoop-env.sh</code></p>"},{"location":"Hadoop/hadoop-02-cluster/#permission-denied","title":"\u274c Permission denied","text":"<p>\u2714 HDFS directories owned by wrong user</p>"},{"location":"Hadoop/hadoop-02-cluster/#16-from-here-to-real-clusters","title":"16. From Here to Real Clusters","text":"<p>Once comfortable, next steps are:</p> <ol> <li>Multi-node Hadoop cluster</li> <li>Hive installation</li> <li>Spark on YARN</li> <li>HDFS performance tuning</li> </ol>"},{"location":"Hadoop/hadoop-02-cluster/#17-mental-model","title":"17. Mental Model","text":"<p>One machine pretending to be a cluster   All Hadoop daemons talking via SSH   Same concepts as production</p>"},{"location":"Hive/hive-01-intro/","title":"Hive-01: Intro","text":"<p>A Practical Introduction to Data Warehousing with Hive</p>"},{"location":"Hive/hive-01-intro/#1-what-is-hive","title":"1. What Is Hive?","text":"<p>Apache Hive is a data warehouse system built on top of Hadoop.</p> <p>It allows you to:</p> <ul> <li>Store large-scale data in HDFS</li> <li>Query data using SQL-like language (HiveQL)</li> <li>Perform batch analytics, not real-time transactions</li> </ul> <p>\ud83d\udc49 Hive is designed for analysis, not for OLTP systems like MySQL.</p>"},{"location":"Hive/hive-01-intro/#2-why-use-hive","title":"2. Why Use Hive?","text":""},{"location":"Hive/hive-01-intro/#21-problems-hive-solves","title":"2.1 Problems Hive Solves","text":"<p>Before Hive:</p> <ul> <li>Hadoop required writing MapReduce code</li> <li>Very hard for SQL users</li> <li>Development was slow</li> </ul> <p>Hive:</p> <ul> <li>Uses SQL instead of Java</li> <li>Automatically converts SQL into distributed jobs</li> <li>Handles TB\u2013PB scale data</li> </ul>"},{"location":"Hive/hive-01-intro/#3-where-hive-fits-in-big-data-architecture","title":"3. Where Hive Fits in Big Data Architecture","text":"<p>Typical architecture:</p> <pre><code>Data Sources\n   \u2193\nHDFS / Object Storage\n   \u2193\nHive (SQL Layer)\n   \u2193\nBI / Reports / Applications\n</code></pre> <p>Hive sits between raw data storage and data analysis tools.</p>"},{"location":"Hive/hive-01-intro/#4-core-components-of-hive","title":"4. Core Components of Hive","text":""},{"location":"Hive/hive-01-intro/#41-hive-metastore","title":"4.1 Hive Metastore","text":"<p>The Metastore stores:</p> <ul> <li>Table names</li> <li>Column definitions</li> <li>Partition information</li> <li>Storage locations</li> </ul> <p>\u26a0\ufe0f Important: Hive tables store metadata only, not the actual data.</p>"},{"location":"Hive/hive-01-intro/#42-hiveql","title":"4.2 HiveQL","text":"<p>Hive uses HiveQL, which looks very similar to SQL:</p> <pre><code>SELECT name, age\nFROM users\nWHERE age &gt; 18;\n</code></pre> <p>But internally:</p> <ul> <li>SQL \u2192 Execution plan</li> <li>Execution plan \u2192 Distributed jobs</li> </ul>"},{"location":"Hive/hive-01-intro/#43-execution-engine","title":"4.3 Execution Engine","text":"<p>Hive converts queries into:</p> <ul> <li>MapReduce (older)</li> <li>Tez</li> <li>Spark (most common today)</li> </ul> <p>You don\u2019t need to write distributed code yourself.</p>"},{"location":"Hive/hive-01-intro/#5-hive-vs-traditional-databases","title":"5. Hive vs Traditional Databases","text":"Feature Hive MySQL Use case Analytics Transactions Data size TB / PB GB Query latency Seconds / minutes Milliseconds Updates Limited Frequent Schema Schema-on-read Schema-on-write"},{"location":"Hive/hive-01-intro/#6-hive-data-model","title":"6. Hive Data Model","text":""},{"location":"Hive/hive-01-intro/#61-database","title":"6.1 Database","text":"<p>A database is just a namespace.</p> <pre><code>CREATE DATABASE analytics;\nUSE analytics;\n</code></pre>"},{"location":"Hive/hive-01-intro/#62-table","title":"6.2 Table","text":"<p>A table maps to a directory in HDFS.</p> <pre><code>CREATE TABLE users (\n    id BIGINT,\n    name STRING,\n    age INT\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY ',';\n</code></pre>"},{"location":"Hive/hive-01-intro/#63-internal-vs-external-tables","title":"6.3 Internal vs External Tables","text":""},{"location":"Hive/hive-01-intro/#internal-table","title":"Internal Table","text":"<ul> <li>Hive manages the data</li> <li>Dropping table deletes data</li> </ul> <pre><code>CREATE TABLE t1 (...);\n</code></pre>"},{"location":"Hive/hive-01-intro/#external-table","title":"External Table","text":"<ul> <li>Data managed outside Hive</li> <li>Dropping table keeps data</li> </ul> <pre><code>CREATE EXTERNAL TABLE t2 (...)\nLOCATION '/data/users';\n</code></pre> <p>\u2705 External tables are recommended for beginners.</p>"},{"location":"Hive/hive-01-intro/#7-loading-data-into-hive","title":"7. Loading Data into Hive","text":""},{"location":"Hive/hive-01-intro/#71-load-from-local-file","title":"7.1 Load from Local File","text":"<pre><code>LOAD DATA LOCAL INPATH '/tmp/users.csv'\nINTO TABLE users;\n</code></pre>"},{"location":"Hive/hive-01-intro/#72-insert-from-another-table","title":"7.2 Insert from Another Table","text":"<pre><code>INSERT INTO TABLE users\nSELECT id, name, age\nFROM users_raw;\n</code></pre>"},{"location":"Hive/hive-01-intro/#8-partitions-and-buckets","title":"8. Partitions and Buckets","text":""},{"location":"Hive/hive-01-intro/#81-partition","title":"8.1 Partition","text":"<p>Partitions split data by directory.</p> <pre><code>CREATE TABLE logs (\n    user_id BIGINT,\n    action STRING\n)\nPARTITIONED BY (dt STRING);\n</code></pre> <p>Directory structure:</p> <pre><code>logs/dt=2026-01-07/\nlogs/dt=2026-01-08/\n</code></pre> <p>Benefits:</p> <ul> <li>Faster queries</li> <li>Less data scanned</li> </ul>"},{"location":"Hive/hive-01-intro/#82-bucket","title":"8.2 Bucket","text":"<p>Buckets split data into files using hash.</p> <pre><code>CLUSTERED BY (user_id) INTO 8 BUCKETS;\n</code></pre> <p>Used mainly for:</p> <ul> <li>Joins</li> <li>Sampling</li> </ul>"},{"location":"Hive/hive-01-intro/#9-common-hiveql-operations","title":"9. Common HiveQL Operations","text":""},{"location":"Hive/hive-01-intro/#91-select","title":"9.1 SELECT","text":"<pre><code>SELECT *\nFROM users\nLIMIT 10;\n</code></pre>"},{"location":"Hive/hive-01-intro/#92-where","title":"9.2 WHERE","text":"<pre><code>SELECT *\nFROM users\nWHERE age &gt;= 18;\n</code></pre>"},{"location":"Hive/hive-01-intro/#93-group-by","title":"9.3 GROUP BY","text":"<pre><code>SELECT age, COUNT(*)\nFROM users\nGROUP BY age;\n</code></pre>"},{"location":"Hive/hive-01-intro/#94-join","title":"9.4 JOIN","text":"<pre><code>SELECT u.name, o.amount\nFROM users u\nJOIN orders o\nON u.id = o.user_id;\n</code></pre>"},{"location":"Hive/hive-01-intro/#10-file-formats-in-hive","title":"10. File Formats in Hive","text":"Format Use Case TEXTFILE Simple, readable ORC Best for Hive PARQUET Best for Spark AVRO Schema evolution <p>\ud83d\udc49 ORC or PARQUET is recommended.</p>"},{"location":"Hive/hive-01-intro/#11-hive-is-not-a-replacement-for-mysql","title":"11. Hive Is Not a Replacement for MySQL","text":"<p>\u274c Do not use Hive for:</p> <ul> <li>Online transactions</li> <li>Frequent updates</li> <li>Row-level deletes</li> </ul> <p>\u2705 Use Hive for:</p> <ul> <li>Reports</li> <li>Batch analytics</li> <li>Data warehousing</li> </ul>"},{"location":"Hive/hive-01-intro/#12-hive-best-practices-for-beginners","title":"12. Hive Best Practices for Beginners","text":"<ol> <li>Always use external tables</li> <li>Always use partitions</li> <li>Prefer ORC / PARQUET</li> <li>Keep business logic out of raw data</li> <li>Design tables for query patterns</li> </ol>"},{"location":"Hive/hive-01-intro/#13-how-hive-is-used-in-real-projects","title":"13. How Hive Is Used in Real Projects","text":"<p>Typical flow:</p> <ol> <li>Ingest data into HDFS</li> <li>Create ODS tables in Hive</li> <li>Clean data into DWD</li> <li>Aggregate into DWS</li> <li>Serve results via ADS</li> </ol> <p>(Exactly the ODS\u2013DWD\u2013DWS\u2013ADS model you just learned)</p>"},{"location":"Hive/hive-01-intro/#14-learning-path-recommendation","title":"14. Learning Path Recommendation","text":"<p>Beginner \u2192 Advanced:</p> <ol> <li>Hive basics &amp; HiveQL</li> <li>Partitions &amp; file formats</li> <li>Data warehouse layering</li> <li>Performance tuning</li> <li>Integration with Spark / Airflow</li> </ol>"},{"location":"Hive/hive-01-intro/#15-final-summary","title":"15. Final Summary","text":"<ul> <li>Hive = SQL on Big Data</li> <li>Hive works on HDFS</li> <li>Hive is for analytics, not OLTP</li> <li>Layered design makes Hive powerful</li> </ul> <p>If you master Hive, you can easily move to:</p> <ul> <li>Spark SQL</li> <li>Presto / Trino</li> <li>Flink SQL</li> </ul>"},{"location":"Hive/hive-02-conf/","title":"Hive-02: Conf","text":""},{"location":"Hive/hive-02-conf/#1-what-is-apache-hive","title":"1. What is Apache Hive?","text":"<p>Apache Hive is a data warehouse system built on top of Hadoop. It allows you to query large datasets stored in HDFS using SQL-like language (HiveQL) instead of writing complex MapReduce code.</p>"},{"location":"Hive/hive-02-conf/#key-features","title":"Key features","text":"<ul> <li>SQL-like syntax (easy to learn)</li> <li>Runs on Hadoop</li> <li>Suitable for batch data analysis</li> <li>Commonly used in big data platforms</li> </ul>"},{"location":"Hive/hive-02-conf/#2-prerequisites","title":"2. Prerequisites","text":"<p>Before installing Hive, make sure the following components are installed and working.</p>"},{"location":"Hive/hive-02-conf/#21-operating-system","title":"2.1 Operating System","text":"<ul> <li>Linux (CentOS / Rocky Linux / Ubuntu)</li> <li>64-bit system</li> </ul>"},{"location":"Hive/hive-02-conf/#22-java-required","title":"2.2 Java (Required)","text":"<p>Hive runs on Java.</p> <pre><code>java -version\n</code></pre> <p>Recommended:</p> <ul> <li>JDK 8 or JDK 11</li> </ul> <p>If Java is not installed, install it first.</p>"},{"location":"Hive/hive-02-conf/#23-hadoop-required","title":"2.3 Hadoop (Required)","text":"<p>Hive depends on Hadoop (HDFS + YARN).</p> <p>Check Hadoop:</p> <pre><code>hadoop version\n</code></pre> <p>Make sure:</p> <ul> <li>HDFS is running</li> <li>You can execute <code>hdfs dfs -ls /</code></li> </ul> <p>If Hadoop is not installed, install Hadoop before Hive.</p>"},{"location":"Hive/hive-02-conf/#3-download-apache-hive","title":"3. Download Apache Hive","text":""},{"location":"Hive/hive-02-conf/#31-choose-a-version","title":"3.1 Choose a Version","text":"<p>It is recommended to match Hive with your Hadoop version.</p> <p>Example:</p> <ul> <li>Hadoop 3.x \u2192 Hive 3.x</li> </ul>"},{"location":"Hive/hive-02-conf/#32-download-hive","title":"3.2 Download Hive","text":"<pre><code>cd /opt\nwget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz\n</code></pre>"},{"location":"Hive/hive-02-conf/#4-install-hive","title":"4. Install Hive","text":""},{"location":"Hive/hive-02-conf/#41-extract-hive","title":"4.1 Extract Hive","text":"<pre><code>tar -zxvf apache-hive-3.1.3-bin.tar.gz\n</code></pre> <ul> <li><code>z</code> - gzip</li> <li><code>x</code> - extract</li> <li><code>v</code> - verbose</li> <li><code>f</code> - file</li> </ul> <p>Rename for convenience:</p> <pre><code>mv apache-hive-3.1.3-bin hive\n</code></pre> <p>Hive directory:</p> <pre><code>/opt/hive\n</code></pre>"},{"location":"Hive/hive-02-conf/#5-configure-environment-variables","title":"5. Configure Environment Variables","text":""},{"location":"Hive/hive-02-conf/#51-edit-etcprofile-or-bashrc","title":"5.1 Edit <code>/etc/profile</code> (or <code>~/.bashrc</code>)","text":"<pre><code>vi /etc/profile\n</code></pre> <p>Add the following:</p> <pre><code># HIVE_HOME\nexport HIVE_HOME=/opt/hive\nexport PATH=$PATH:$HIVE_HOME/bin\n</code></pre> <p>Apply:</p> <pre><code>source /etc/profile\n</code></pre> <p>Verify:</p> <pre><code>hive --version\n</code></pre>"},{"location":"Hive/hive-02-conf/#52-resolving-library-conflicts","title":"5.2 Resolving Library Conflicts","text":"<p>Hive includes a logging implementation that often conflicts with Hadoop's version. To prevent startup errors, rename the following file in the Hive <code>lib</code> folder:</p> <pre><code>cd $HIVE_HOME/lib/\nmv log4j-slf4j-impl-2.17.1.jar log4j-slf4j-impl-2.17.1.jar.bak\n</code></pre>"},{"location":"Hive/hive-02-conf/#6-configure-hive","title":"6. Configure Hive","text":"<p>Hive configuration files are located in:</p> <pre><code>$HIVE_HOME/conf\n</code></pre>"},{"location":"Hive/hive-02-conf/#61-create-hive-sitexml","title":"6.1 Create <code>hive-site.xml</code>","text":"<p>Hive does not provide this file by default.</p> <pre><code>cd $HIVE_HOME/conf\ncp hive-default.xml.template hive-site.xml\n</code></pre>"},{"location":"Hive/hive-02-conf/#62-configure-metastore-very-important","title":"6.2 Configure Metastore (Very Important)","text":"<p>Hive needs a metastore to store table metadata.</p> <p>There are three types:</p> <ul> <li>Embedded Derby (default, only for testing)</li> <li>MySQL / MariaDB (recommended)</li> <li>PostgreSQL</li> </ul> <p>For beginners, MySQL is recommended.</p>"},{"location":"Hive/hive-02-conf/#7-install-and-configure-mysql-metastore","title":"7. Install and Configure MySQL (Metastore)","text":""},{"location":"Hive/hive-02-conf/#71-install-mysql","title":"7.1 Install MySQL","text":"<pre><code>yum install mysql-server -y\nsystemctl start mysqld\nsystemctl enable mysqld\n</code></pre>"},{"location":"Hive/hive-02-conf/#72-create-hive-metastore-database","title":"7.2 Create Hive Metastore Database","text":"<pre><code>mysql -u root -p\n</code></pre> <p>Inside MySQL:</p> <pre><code>CREATE DATABASE hive_metastore;\nCREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive';\nGRANT ALL PRIVILEGES ON hive_metastore.* TO 'hive'@'localhost';\nFLUSH PRIVILEGES;\nEXIT;\n</code></pre>"},{"location":"Hive/hive-02-conf/#73-add-mysql-jdbc-driver","title":"7.3 Add MySQL JDBC Driver","text":"<p>Download MySQL Connector:</p> <pre><code>wget https://downloads.mysql.com/archives/get/p/3/file/mysql-connector-java-8.0.25.tar.gz\n</code></pre> <p>Extract:</p> <pre><code>tar -zxvf mysql-connector-java-8.0.25.tar.gz\n</code></pre> <p>Copy JAR to the Hive <code>lib</code> directory so Hive can communicate with the MySQL database.</p> <pre><code>cp mysql-connector-java-8.0.25/mysql-connector-java-8.0.25.jar $HIVE_HOME/lib/\n</code></pre>"},{"location":"Hive/hive-02-conf/#8-edit-hive-sitexml","title":"8. Edit <code>hive-site.xml</code>","text":"<p>Open:</p> <pre><code>vi $HIVE_HOME/conf/hive-site.xml\n</code></pre> <p>Add the following properties inside <code>&lt;configuration&gt;</code>:</p> <pre><code>&lt;property&gt;\n  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n  &lt;value&gt;jdbc:mysql://localhost:3306/hive_metastore?useSSL=false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n  &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n  &lt;value&gt;hive&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n  &lt;value&gt;hive&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n&lt;/property&gt;\n</code></pre> <p>\u6bd5\u4e1a\u5b9e\u4e60\u7684\u914d\u7f6e\u5982\u4e0b\uff1a</p> <pre><code>&lt;?xml version=\"1.0\"?&gt;\n&lt;?xml-stylesheet type=\"1.0\" href=\"configuration.xsl\"?&gt;\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n        &lt;value&gt;jdbc:mysql://hadoop01:3306/metastore?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&amp;amp;allowPublicKeyRetrieval=true&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n        &lt;value&gt;com.mysql.cj.jdbc.Driver&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n        &lt;value&gt;root&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n        &lt;value&gt;000000&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;\n        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;\n    &lt;/property&gt;\n\n    &lt;property&gt;\n        &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;\n        &lt;value&gt;hadoop01&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.cli.print.header&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;\n        &lt;value&gt;true&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre>"},{"location":"Hive/hive-02-conf/#9-initialize-hive-metastore","title":"9. Initialize Hive Metastore","text":""},{"location":"Hive/hive-02-conf/#91-create-warehouse-directory-in-hdfs","title":"9.1 Create Warehouse Directory in HDFS","text":"<pre><code>hdfs dfs -mkdir -p /user/hive/warehouse\nhdfs dfs -chmod -R 777 /user/hive\n</code></pre>"},{"location":"Hive/hive-02-conf/#92-initialize-schema","title":"9.2 Initialize Schema","text":"<p>Log into MySQL and create the database:</p> <pre><code>mysql -uroot -p\n&gt; CREATE DATABASE metastore;\n&gt; QUIT;\n</code></pre> <p>Then initializa schema:</p> <pre><code>schematool -initSchema -dbType mysql -verbose\n</code></pre> <p>If successful, you will see:</p> <pre><code>schemaTool completed\n</code></pre>"},{"location":"Hive/hive-02-conf/#93-fixing-character-encoding-for-comments","title":"9.3 Fixing Character Encoding for Comments","text":"<p>By default, Hive metastore uses <code>Latin1</code>, which causes issues when using Chinese characters in table or column comments. Modify the following tables in MySQL to support <code>UTF-8</code>:</p> <pre><code>USE metastore;\n\n-- Modify Column Comment Encoding\nALTER TABLE COLUMNS_V2 MODIFY COLUMN COMMENT VARCHAR(256) CHARACTER SET utf8;\n\n-- Modify Table Parameter Encoding\nALTER TABLE TABLE_PARAMS MODIFY COLUMN PARAM_VALUE MEDIUMTEXT CHARACTER SET utf8;\n</code></pre>"},{"location":"Hive/hive-02-conf/#10-start-hive","title":"10. Start Hive","text":""},{"location":"Hive/hive-02-conf/#101-start-hive-shell","title":"10.1 Start Hive Shell","text":"<p>Ensure your Hadoop cluster is running before starting the Hive client:</p> <pre><code># Start Hadoop\nhdp.sh start\n\n# Launch Hive\nhive\n</code></pre> <p>You should see:</p> <pre><code>hive&gt;\n</code></pre>"},{"location":"Hive/hive-02-conf/#102-test-hive","title":"10.2 Test Hive","text":"<pre><code>SHOW DATABASES;\n</code></pre> <p>Create a database:</p> <pre><code>CREATE DATABASE testdb;\nSHOW DATABASES;\n</code></pre>"},{"location":"Hive/hive-02-conf/#11-common-problems-for-beginners","title":"11. Common Problems for Beginners","text":""},{"location":"Hive/hive-02-conf/#111-javalangnosuchmethoderror","title":"11.1 <code>java.lang.NoSuchMethodError</code>","text":"<ul> <li>Version conflict between Hadoop and Hive</li> <li>Ensure compatible versions</li> </ul>"},{"location":"Hive/hive-02-conf/#112-cannot-connect-to-metastore","title":"11.2 Cannot Connect to Metastore","text":"<p>Check:</p> <ul> <li>MySQL is running</li> <li>JDBC driver is in <code>$HIVE_HOME/lib</code></li> <li>Database user/password is correct</li> </ul>"},{"location":"Hive/hive-02-conf/#113-permission-denied-in-hdfs","title":"11.3 Permission Denied in HDFS","text":"<pre><code>hdfs dfs -chmod -R 777 /user/hive\n</code></pre>"},{"location":"Hive/hive-02-conf/#12-hive-configuration-files-summary","title":"12. Hive Configuration Files Summary","text":"File Purpose hive-site.xml Main configuration hive-env.sh Hive environment variables hive-log4j2.properties Logging core-site.xml Hadoop config hdfs-site.xml HDFS config"},{"location":"Hive/hive-02-conf/#13-next-steps","title":"13. Next Steps","text":"<p>After installation, you can learn:</p> <ul> <li>HiveQL basics (<code>SELECT</code>, <code>WHERE</code>, <code>GROUP BY</code>)</li> <li>External vs Managed tables</li> <li>ODS / DWD / DWS / ADS architecture</li> <li>Hive on Tez / Spark</li> </ul>"},{"location":"Hive/hive-02-conf/#14-quick-checklist","title":"14. Quick Checklist","text":"<p>\u2705 Java installed \u2705 Hadoop running \u2705 MySQL metastore configured \u2705 Warehouse directory created \u2705 <code>hive</code> command works</p>"},{"location":"Hive/hive-03-layers/","title":"Hive-03: Layering","text":"<p>A Beginner\u2019s Guide to Data Warehouse</p>"},{"location":"Hive/hive-03-layers/#1-why-do-we-need-data-warehouse-layers","title":"1. Why Do We Need Data Warehouse Layers?","text":"<p>When building a data warehouse with Hive, raw data usually comes from many sources:</p> <ul> <li>Application databases (MySQL, PostgreSQL)</li> <li>Logs (web, app, server logs)</li> <li>External systems (Kafka, APIs)</li> </ul> <p>If we put all data into Hive without structure, we quickly face problems:</p> <ul> <li>Data is messy and inconsistent</li> <li>Queries become slow and complex</li> <li>Business logic is duplicated everywhere</li> <li>Hard to maintain and scale</li> </ul> <p>\ud83d\udc49 Solution: Use a layered data warehouse architecture.</p> <p>The most common and practical layering model in Hive is:</p> <pre><code>ODS \u2192 DWD \u2192 DWS \u2192 ADS\n</code></pre> <p>Each layer has a clear responsibility.</p>"},{"location":"Hive/hive-03-layers/#2-overview-of-odsdwddwsads","title":"2. Overview of ODS\u2013DWD\u2013DWS\u2013ADS","text":"Layer Full Name Purpose ODS Operational Data Store Store raw data DWD Data Warehouse Detail Cleaned, detailed data DWS Data Warehouse Summary Aggregated, theme-based data ADS Application Data Service Data for reports &amp; applications <p>Think of it as a data refinement pipeline:</p> <pre><code>Raw \u2192 Clean \u2192 Aggregate \u2192 Serve\n</code></pre>"},{"location":"Hive/hive-03-layers/#3-ods-operational-data-store","title":"3. ODS (Operational Data Store)","text":""},{"location":"Hive/hive-03-layers/#31-what-is-ods","title":"3.1 What Is ODS?","text":"<p>ODS stores raw data as close to the source as possible.</p> <ul> <li>No or minimal processing</li> <li>Keeps original structure</li> <li>Used as a data backup and audit layer</li> </ul>"},{"location":"Hive/hive-03-layers/#32-characteristics","title":"3.2 Characteristics","text":"<ul> <li>One table \u2248 one source table</li> <li>Fields match source fields</li> <li>Usually partitioned by date</li> <li>Append-only (no updates)</li> </ul>"},{"location":"Hive/hive-03-layers/#33-example","title":"3.3 Example","text":"<p>Source: MySQL <code>user</code> table ODS table:</p> <pre><code>CREATE TABLE ods_user (\n    id BIGINT,\n    name STRING,\n    email STRING,\n    create_time STRING\n)\nPARTITIONED BY (dt STRING)\nSTORED AS PARQUET;\n</code></pre> <p>Load data:</p> <pre><code>INSERT OVERWRITE TABLE ods_user PARTITION (dt='2026-01-07')\nSELECT id, name, email, create_time\nFROM mysql_user;\n</code></pre>"},{"location":"Hive/hive-03-layers/#34-what-not-to-do-in-ods","title":"3.4 What NOT to Do in ODS","text":"<p>\u274c Do not:</p> <ul> <li>Change field meanings</li> <li>Apply business rules</li> <li>Join tables</li> </ul> <p>ODS = \u201cWhat the source gave me\u201d</p>"},{"location":"Hive/hive-03-layers/#4-dwd-data-warehouse-detail","title":"4. DWD (Data Warehouse Detail)","text":""},{"location":"Hive/hive-03-layers/#41-what-is-dwd","title":"4.1 What Is DWD?","text":"<p>DWD is the cleaned and standardized detail layer.</p> <p>Here we:</p> <ul> <li>Clean dirty data</li> <li>Standardize formats</li> <li>Apply basic business logic</li> </ul>"},{"location":"Hive/hive-03-layers/#42-typical-operations","title":"4.2 Typical Operations","text":"<ul> <li>Remove duplicates</li> <li>Handle null values</li> <li>Convert data types</li> <li>Normalize codes</li> <li>Add derived fields</li> </ul>"},{"location":"Hive/hive-03-layers/#43-example","title":"4.3 Example","text":"<p>From <code>ods_user</code> \u2192 <code>dwd_user</code></p> <pre><code>CREATE TABLE dwd_user (\n    user_id BIGINT,\n    user_name STRING,\n    email STRING,\n    create_date DATE\n)\nPARTITIONED BY (dt STRING)\nSTORED AS PARQUET;\nINSERT OVERWRITE TABLE dwd_user PARTITION (dt='2026-01-07')\nSELECT\n    id AS user_id,\n    name AS user_name,\n    email,\n    TO_DATE(create_time) AS create_date\nFROM ods_user\nWHERE dt='2026-01-07'\n  AND id IS NOT NULL;\n</code></pre>"},{"location":"Hive/hive-03-layers/#44-key-rule","title":"4.4 Key Rule","text":"<p>DWD data should be:</p> <ul> <li>Clean</li> <li>Consistent</li> <li>Reusable</li> </ul> <p>DWD is often the most important layer.</p>"},{"location":"Hive/hive-03-layers/#5-dws-data-warehouse-summary","title":"5. DWS (Data Warehouse Summary)","text":""},{"location":"Hive/hive-03-layers/#51-what-is-dws","title":"5.1 What Is DWS?","text":"<p>DWS is the aggregation layer, organized by business themes.</p> <p>Examples of themes:</p> <ul> <li>User behavior</li> <li>Orders</li> <li>Revenue</li> <li>Traffic</li> </ul>"},{"location":"Hive/hive-03-layers/#52-purpose","title":"5.2 Purpose","text":"<ul> <li>Reduce repeated aggregation</li> <li>Improve query performance</li> <li>Provide unified metrics</li> </ul>"},{"location":"Hive/hive-03-layers/#53-example","title":"5.3 Example","text":"<p>User daily summary</p> <pre><code>CREATE TABLE dws_user_day (\n    dt STRING,\n    new_user_count BIGINT,\n    total_user_count BIGINT\n)\nSTORED AS PARQUET;\nINSERT OVERWRITE TABLE dws_user_day\nSELECT\n    dt,\n    COUNT(user_id) AS new_user_count,\n    SUM(COUNT(user_id)) OVER (ORDER BY dt) AS total_user_count\nFROM dwd_user\nGROUP BY dt;\n</code></pre>"},{"location":"Hive/hive-03-layers/#54-design-tips","title":"5.4 Design Tips","text":"<ul> <li>Usually wide tables</li> <li>One table serves many downstream queries</li> <li>Metrics are stable and reusable</li> </ul>"},{"location":"Hive/hive-03-layers/#6-ads-application-data-service","title":"6. ADS (Application Data Service)","text":""},{"location":"Hive/hive-03-layers/#61-what-is-ads","title":"6.1 What Is ADS?","text":"<p>ADS serves data directly to applications and reports.</p> <p>Consumers:</p> <ul> <li>BI tools (Tableau, Power BI)</li> <li>Dashboards</li> <li>APIs</li> <li>Management reports</li> </ul>"},{"location":"Hive/hive-03-layers/#62-characteristics","title":"6.2 Characteristics","text":"<ul> <li>Highly aggregated</li> <li>Business-oriented</li> <li>Small data volume</li> <li>Fast query speed</li> </ul>"},{"location":"Hive/hive-03-layers/#63-example","title":"6.3 Example","text":"<p>Daily dashboard table</p> <pre><code>CREATE TABLE ads_dashboard (\n    dt STRING,\n    new_users BIGINT,\n    total_users BIGINT\n)\nSTORED AS PARQUET;\nINSERT OVERWRITE TABLE ads_dashboard\nSELECT\n    dt,\n    new_user_count,\n    total_user_count\nFROM dws_user_day;\n</code></pre>"},{"location":"Hive/hive-03-layers/#64-key-rule","title":"6.4 Key Rule","text":"<p>ADS = \u201cWhat the business wants to see\u201d</p>"},{"location":"Hive/hive-03-layers/#7-data-flow-summary","title":"7. Data Flow Summary","text":"<pre><code>ODS  \u2192  DWD  \u2192  DWS  \u2192  ADS\nRaw     Clean    Aggregate   Serve\n</code></pre> Layer Changes Allowed ODS Almost none DWD Cleaning &amp; standardization DWS Aggregation &amp; metrics ADS Business presentation"},{"location":"Hive/hive-03-layers/#8-common-best-practices","title":"8. Common Best Practices","text":""},{"location":"Hive/hive-03-layers/#81-naming-conventions","title":"8.1 Naming Conventions","text":"<ul> <li>Database: <code>ods</code>, <code>dwd</code>, <code>dws</code>, <code>ads</code></li> <li>Table prefix: same as layer name</li> </ul> <p>Example:</p> <pre><code>ods_user\ndwd_user\ndws_user_day\nads_dashboard\n</code></pre>"},{"location":"Hive/hive-03-layers/#82-partition-strategy","title":"8.2 Partition Strategy","text":"<ul> <li>Most tables partition by date (<code>dt</code>)</li> <li>Improves query performance</li> <li>Simplifies data lifecycle management</li> </ul>"},{"location":"Hive/hive-03-layers/#83-avoid-these-mistakes","title":"8.3 Avoid These Mistakes","text":"<p>\u274c Putting business logic in ODS \u274c Re-aggregating data in ADS \u274c Skipping DWD and directly using ODS \u274c Making ADS too complex</p>"},{"location":"Hive/hive-03-layers/#9-how-beginners-should-practice","title":"9. How Beginners Should Practice","text":"<ol> <li>Start with one business process (e.g., user registration)</li> <li>Build:<ul> <li>1 ODS table</li> <li>1 DWD table</li> <li>1 DWS table</li> <li>1 ADS table</li> </ul> </li> <li>Write SQL step by step</li> <li>Focus on clarity, not optimization first</li> </ol>"},{"location":"Hive/hive-03-layers/#10-final-takeaway","title":"10. Final Takeaway","text":"<ul> <li>ODS: raw, unchanged data</li> <li>DWD: clean, standardized detail data</li> <li>DWS: aggregated, theme-based data</li> <li>ADS: business-facing result data</li> </ul> <p>If you understand why each layer exists, Hive data warehouse design becomes clear, scalable, and maintainable.</p>"}]}